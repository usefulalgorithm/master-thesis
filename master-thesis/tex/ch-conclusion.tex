\chapter{Conclusion}\label{ch:conclusion}

In this work, we applied the \emph{probably-approximate-correct} (PAC) learning framework and its guarantee to the task of checking the validity of program assertions. More specifically, we propose the $\PAC$ guarantee, which is based on the guarantee provided by PAC learning. We proposed two approaches, the \emph{learning-based} and the \emph{sampling-based} approach, to handle the task. 

Both the learning-based and the sampling-based approaches utilize the $\PAC$ guarantee to provide correctness insurance of the program-under-test. The learning-based procedure makes use of a modified version of the automata learning framework mentioned in \cite{Angluin87}, which can handle the scenario when a target automaton contains an error, and replaces the checking of exact equivalence with sampling in order to provide programs with the $\PAC$ness guarantee. Moreover, the learning-based procedure incorporates several additional tests to enhance the performance. On the other hand, the sampling-based procedure directly takes the requirements $\PAC$ guarantee to calculate the number of samples necessary to provide programs with correctness insurance. It is also possible to execute the sampling-based procedure with several search strategies of the sampler.

We implemented a prototype named \PACMAN based on the two procedures. \PACMAN supports two modes, the \emph{synthesize-and-verify} mode and the \emph{direct} mode, implementing the learning-based and the sampling-based procedure respectively. We utilized several third-party components, such as software model checkers such as \textsc{CPAchecker} \cite{cpachecker} and CBMC \cite{cbmc}, the concolic tester \textsc{Crest} \cite{crest}, and the libraries CIL \cite{cil}, \textsc{libALF} \cite{libalf} and \textsc{libAMoRE++} \cite{libamore++}. In addition, we conducted experiments to determine the best learning algorithm and search strategy of the concolic tester with respect to performance. We submitted \PACMAN in \emph{direct} mode to participate in the recursive and array-reach subcategories of SV-COMP 2016 \cite{svcomp16}, and achieved promising results. Moreover, we submitted the learning-based procedeure to ICSE 2016 \cite{icse2016}, and was successfully accepted. 

Finally, it is possible to extend our approaches with different enhancement methods, such as applying heuristics, use sampling mechanisms with different distributions, etc. It is also viable to modify our learning-based approach so that it constructs automata approximating programs' function call graph instead of the control flow graph. With this work, we hope to find a balance between scalability and correctness guarantees by applying the PAC guarantee on the learning-based and sampling-based procedures, thus blurring the dichotomy between program testing and formal methods. 


\begin{comment}
% Extension: using program analyzer + bounded model checking

The number of iterations is perhaps the most important factor in our
recursion analysis technique (Table~\ref{table:experiments}) as it would
determine how many times of unwinding are applied.
We find that \textsc{CPAchecker} performs poorly when checking programs
that is unwound many times.
We however do not enable the more efficient block encoding in
\textsc{CPAchecker} for the ease of implementation.
One can improve the performance of our algorithm with the efficient but
complicated block encoding.
A bounded analyzer may also speed up the verification of bounded properties.

Our algorithm extracts function summaries from inductive invariants.
There are certainly many heuristics to optimize the computation of
function summaries.
For instance, some program analyzers return error traces when properties fail.
In particular, a valuation of formal parameters is obtained when
\textmd{CheckSummary} (Algorithm~\ref{algorithm:check-summary}) returns $\FF$.
If the valuation is not possible in the $\fun{main}$ function, one can use
its inductive invariant to refine function summaries.
We in fact exploit error traces computed by \textsc{CPAchecker} in the
implementation.

Another improvement on our algorithm is on selecting locations for extracting
inductive invariants.
In Algorithm~\ref{algorithm:mark-locations}, we select only outermost pairs of
locations for calls to the same function.
This is based on the observation that the unwound bodies of these function calls
contain more execution paths,
and hence their behaviors should be closer to the original function.
However, the extracted invariants in $s_i$ may be too precise to those certain
function calls and result in too coarse summary candidates constructed by
implication connective,
and consequently the candidates can not pass \textmd{CheckSummary} due to inner
function calls are not properly approximated.
Therefore, heuristics that select some locations of inner function calls may help
compute summary candidates with better quality.

Further enhancement and application of our work would be supporting recursive
data structures and verifying operations on the data structure.
Common recursive functions in real world C program mostly are simple operations
on recursive data structures, such as list, tree, graph, etc.,
but our prototype currently only verifies integer programs.
If our approach is adapted to handle data structures, a dedicated logic is
required for describing data structure as well as semantic for the operations,
and a \method{BasicAnalyzer} providing inductive invariants in such logic surely
is necessary.
Our primitive study reveals several difficulties on applying our approach with
separation logic~\cite{Reynolds02} and the \textsc{Thor}
analyzer~\cite{MagillTLT08}.

One major obstacle comes from \textmd{ComputeSummary} and \textmd{CheckSummary}.
In our method, implication connective and universal quantification are used to
derive summary.
These operators are intuitive in propositional and Presburger arithmetic logic.
By contrast, there is no such operator on separation logic.
Consequently, we tried to devise different methods for compute and check
summaries in order to avoid the usage of the operators.
A possible workaround for this problem is to use multiple formulae instead of
one single formula to represent one function summary.

Another problem arises from the expression allowed in $\mathtt{assume}$ and
$\mathtt{assert}$ command.
Our program model does not introduce a separated \emph{assertion language} for
specifying preconditions and postconditions,
and this is to comply with the assertions in C language.
The drawback of this choice is that the expressiveness and grammar of allowed
boolean expression in program is limited,
and hence not necessarily consistent with the logic in \method{BasicAnalyzer}.
To precisely describe a recursive data structure, we tried to introduce
annotations recognized by \method{BasicAnalyzer} into C programs.
In our study, we actually used the assertion language defined by \textsc{Thor}
to annotate C programs,
and much more efforts are needed to understand the underlying analyzer,
not to mention the transformation on the formulae.

Finally, reducing program features via program transformation techniques is the
core concept of our work.
We believe this idea can be applied to deal with other kinds of program
features, such as pointers, unbounded integral types, procedure calls through
dynamic look up, absent code due to external libraries, etc.
Program analysis tools can rely on one efficient and simple core
\method{BasicAnalyzer} and modular components doing transformation for different
program features.
The development tasks for the tools could be easily divided and dispatched,
and optimization for different components could be independent and loosely
coupled.
Overall, we believe the concept could speed up the process of research as well
as implementation on program analysis.

\end{comment}