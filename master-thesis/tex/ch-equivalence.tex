\chapter{Resolving Equivalence Queries By Sampling}\label{ch:eq}

This chapter discusses how we enabled the teacher to answer equivalence queries in order to provide $\PAC$ness guarantee, as defined in Chapter \ref{ch:PAC}. Given a probability distribution $D$ over the set of feasible decision vectors $\DEC(\Pi)$, we can use $D$ to give a formal definition on the quality of the candidate automaton $C$. In particular, we use the probability of a decision vector, which was chosen from $\DEC(\Pi)$ according to $D$, is actually contained in $C$. 

A sampling mechanism offering such a distribution must satisfy the following conditions: 
\begin{enumerate}
	\item Only decisions in $\DEC(\Pi)$ are sampled.\label{item:sample_cond_1}
	\item The samples are \emph{independent and identically distributed} (IID). That is, the distribution is fixed, and the probability of sampling a particular element does not depend on the previous samples. \label{item:sample_cond_2}
\end{enumerate}
In the following paragraph, we introduce the notion of \emph{random input sampling}. We treat all nondeterministic choices and formal parameters of the program as input variables, and we assume the input variables are over finite domains. Each set of initial values of input variables yields a path in the CFG of the program. With the above statements in mind, random input sampling works by (1) uniformly picking at random from a set of initial values for input variables, and (2) obtaining the corresponding decision vector by traversing the CFG based on the picked values. The sampling mechanism forms a distribution such that the probability of picking a decision vector $d$ is proportional to the number of program paths corresponding to $d$. 

The issue of random input sampling is that it suffers from the fact that the coverage of input values is not a good approximation of program path coverage. Depending on the sizes of the program variables' input domains, some paths might have only a negligible probability of being selected. Consider the case when given two 32-bit integers  \texttt{x} and \texttt{y}, the probability of taking the \textit{true} branch in the test \lstinline|x == 0 && y == 0| is equal to $2^{-64}$. The situation gets even worse for input variables over unbounded domains. Even with an extremely high coverage rate of input variables' values, many paths may still remain unexplored, while the others are traversed repeatedly. To get a sampling mechanism with a better distribution over program paths, we developed a technique that randomly explores program's paths using a concolic tester \cite{GodefroidKS05,SenMA05,BurnimS08}, an efficient method for exploring decision vectors corresponding to rare paths. 

\section{Applying Concolic Tester}\label{sec:apply_concolic}

As stated in Section \ref{sec:concolic_testing}, concolic testing is a testing technique that explores paths in a program's CFG. In addition to the usual usage of concolic testers, we adopt the concept of batched samples. A \emph{batched sample} is defined as a set of decision vectors of the size $k$, where $k$ is a given parameter, obtained by a concolic tester from exploring $k$ paths using its search strategy. We denote $D_k$ the distribution over elements of $(\Sigma^\ast)^k$ obtained in this method. Our procedure restarts the concolic tester after taking a batched sample -- this ensures that the probability of taking each batched sample remains the same during the procedure (we assume that the concolic tester does not keep state information between the restarts), thus guaranteeing the distribution IID. Therefore, the distribution $D_k$ meets condition \ref{item:sample_cond_2} defined in the previous section. The principal functioning of concolic testers guarantees that condition \ref{item:sample_cond_1} is also met. 