\chapter{Implementation}\label{ch:implementation}

We created a prototype tool \PACMAN that implements the two procedures described in this paper. More specifically, \PACMAN supports two modes: \emph{synthesize-and-verify} and \emph{direct}, corresponding to the learning-based and the sampling-based procedure respectively. In the following sections, detailed descriptions of each procedure will be given.

\section{The Learning-Based Procedure}\label{sec:impl_learning}

In the \emph{synthesize-and-verify} mode, we utilize several third-party libraries and tools. First, it uses CIL (C Intermediate Language) \cite{NeculaMRW02}, \cite{cil} to convert the program-under-test, which is a C program, to a set of CFGC's. We then construct the error trace pushdown automaton $B_P$ based on the set of CFGC's. Furthermore, we use the library \textsc{libAMoRE++} \cite{MatzMPTV95}, \cite{BolligKKLNP10} \cite{libamore++} to perform operations on automata, i.e. checking for membership and emptiness, or computing intersections. 

For learning automata, we use the implementation of various learning algorithms within the \textsc{libALF} library \cite{BolligKKLNP10} \cite{libalf}. Membership queries are discharged using a concolic tester, mentioned as an alternative option in Chapter \ref{ch:mem}. Given a decision vector, \PACMAN uses the CFG of the program to generate a path corresponding to the decision vector. The path then is passed in the form of a sequence of program statements to a readily available software model checker to check the path's feasibility. In our implementation, we've mainly used the software model checker \textsc{CPAchecker} \cite{BeyerK11} \cite{cpachecker}, but it is interchangeable with other software model checkers such as CBMC \cite{ClarkeKL04} \cite{cbmc}, \textsc{Predator} \cite{DudkaPV11} \cite{predator} and \textsc{SeaHorn} \cite{GurfinkelKKN15} \cite{seahorn}.

On the other hand, equivalence queries are discharged by utilizing a modified version of the concolic tester \textsc{Crest} \cite{BurnimS08} \cite{crest} to generate a batch of $k$ decision vectors, as described in Chapter \ref{ch:eq}. As \textsc{Crest} might fail to generate the decision vector of a program execution due to abnormal termination of the execution, we modified \textsc{Crest} to take a prefix of the execution in finite length in this case. An issue of \textsc{Crest} we encountered is that when proposed a condition comprising Boolean connectives, it expands the condition into a cascade of \texttt{if} statements corresponding to the Boolean expression, thus increasing the size of the program and making it even harder to learn. We addressed this by modifying \textsc{Crest} so that it can process conditions containing Boolean connectives without expanding them, and consequently increased the performance and precision of the analysis. 

We also implemented the following three optimizations to improve the performance of \PACMAN under synthesize-and-verify mode.

\subsection{Intersection with Bad Automaton}\label{subsec:intersection_B}

Recall that the modified version of the learning algorithm (described in Section \ref{sec:learning_program_verify}) first checks the emptiness of the intersection between the language of the candidate automaton, $L(C)$, and the bad language, $L(B_P)$. However, checking the emptiness of a PDA is more difficult than that of a finite automaton. To speed up the emptiness checking mechanism, we build a finite automaton $B_O$ that overapproximates the error language, and checks whether $L(C) \cap L(B_O) = \emptyset$ -- which is basically checking the emptiness of a finite automaton. We only check if $L(C) \cap L(B_P) = \emptyset$ when $L(C) \cap L(B_O) \neq \emptyset$. 

\subsection{Counterexample from the Learning Algorithm}\label{subsec:cex_learning_alg}

When the mechanical teacher responds to an equivalence by presenting a counterexample $c$, automata learning algorithms usually do not guarantee that $c$ is not a valid counterexample in the next candidate automaton. In our preliminary experiments, we discovered that it is quite often the case that the mechanical teacher returns the same counterexample for several consecutive iterations. Therefore, we decided to check whether $c$ is still a valid counterexample via a membership query for the learning algorithm before proceeding to the emptiness checking. If $c$ is indeed a valid counterexample, it will be returned to the learning algorithm to refine the candidate automaton. 

\subsection{Handling Membership Queries}\label{subsec:handle_mem}

The main bottleneck of our approach is the time spent on membership queries. In our implementaion, the software model checker \textsc{CPAchecker} is used to check for the paths' feasibility. For each membership query, if we invoke \textsc{CPAchecker} via system call, a Java virtual machine will be created, and the components of \textsc{CPAchecker} will have to be loaded again -- which is very time consuming. To make membership queries more effricient, we modified \textsc{CPAchecker} to run in a server mode so that it can check more than a single path without being re-invoked. 

\section{The Sampling-Based Procedure}\label{sec:impl_sampling}

As for the \emph{direct} mode, the implementation is more straightforward. We used the modified version of \textsc{Crest} described in Section \ref{sec:impl_learning}, and use it to retrieve program traces as samples. If, within a given time limit, we obtained $r$ number of samples such that with $\epsilon$ and $\delta$ given by the user, $(1-\epsilon)^r < 1-\delta$, then we report the program to be $\PAC$. However, if no bug is found within the time limit, but the number of samples obtained during the procedure $r'$ does not meet the requirements of $\PAC$ guarantee, i.e. $(1-\epsilon)^{r'} \geq 1 - \delta$, then we cannot deduce the $\PAC$ness of the program. In this case, we report the program as Unknown.

We submitted the direct mode of \textsc{Pac-Man} to participate in SV-COMP 2016 \cite{svcomp16}. In order to meet the competition's requirements, we augmented several components to our implementation. In SV-COMP's rules, an error witness is required when an error is encountered during verification process. We utilized an OCaml program along with a script written in bash to translate the output of \textsc{Crest} into an execution trace in C, which leads to the error location. However, this execution trace contains some redundant statements and duplicated function declarations. Futhurmore, SV-COMP 2016 requires the witnesses be turned in as a GraphML \cite{graphml} file instead of a C file. Therefore, we wrote a lightweight program in C++ to simplify the execution trace, and transform it to a file in GraphML. 