\chapter{PAC Automata Learning}\label{ch:PAC}

In this chapter, we will explain the PAC automata learning algorithm that we use to find an approximation to $\DEC(\Pi)$. Classical PAC automata learning algorithm cannot be used directly for the purpose of program verification (proving correctness of a program) -- the algorithm should be able to handle the case when bugs are present in the program. The classical PAC automata learning algorithm was obtained from modifying the requirement of exact automata learning algorithm \cite{Angluin88}. Our PAC automata learning algorithm does the modification in a similar fashion. We first describe the classical "exact" automata learning algorithm of regular languages, then describe how to modify it for our purpose, and finally explain how to relax the requirement of an exact automata learning algorithm to infer an approximation to $\DEC(\Pi)$. 

\section{Exact Learning of Regular Languages}\label{sec:exact}

Suppose $R$ is a \emph{target} regular language whose description is not directly accessible. \emph{Automata learning} algorithms \cite{Angluin87, BolligHKL09, RivestS93, KearnsV94} infer automatically a finite automaton $A_R$ that recognizes $R$. The setting of an online learning algorithm assumes a \emph{teacher} that has access to $R$ and can answer the following queries:
\begin{itemize}
	\item Membership query ($Mem(w)$): whether the word $w$ a member of $R$, i.e., $w \in R$. The teacher should reply either yes or no. 
	\item Equivalence query ($Equ(C)$): whether the language of finite automaton $C$ equivalent to $R$, i.e., $L(C) = R$. The teacher should either reply yes, or report an counterexample to the equality (a word in the symmetric difference of $L(C)$ and $R$, $L(C) \oplus R$, that is).	
\end{itemize}
The learning algorithm will then construct a finite automaton $A_R$ such that $L(A_R) = R$ through interacting with the teacher. Such mechanism works iteratively: in each iteration, it presents membership queries ($Mem(w)$)to the teacher in order to get information about $R$. Using the results of the queries, it proceeds by constructing a candidate automaton $C$, and finally makes an equivalence query ($Equ(C)$). If $L(C) = R$, the algorithm terminates with $C$ being the resulting finite automaton $A_R$. Otherwise, the teacher returns a word $w$ distinguishing $L(C)$ from the target language $R$. The learning algorithm uses $w$ to update the information about $R$ before moving on to the next iteration. The mentioned learning algorithms are guaranteed to find a resulting finite automaton $A_R$ recognizing $R$ using queries polynomial to the number of states of the minimal DFA recognizing $R$. In the rest of the paper, we would denote "online automata learning" simply as "automata learning", since the automata learning algorithms in this paper are all online algorithms. 

\section{Learning for Program Verification}\label{sec:learning_program_verify}

Under the context of program verification, it may be the case that $\DEC(\Pi) \cap L(B) \neq \emptyset$. In such a case, our procedure should report a feasible error path in the program. This is very similar to the scenario of \emph{learning-based verification} \cite{CobleighGP03,ChenFCTW09}, where the learning algorithm is modified to return a counterexample in case the system contains an error. We modified the learning algorithm in a similar way. When the classical learning algorithm poses an equivalence query $Equ(C)$, we first perform a check on whether there exists a decision vector $c$ such that $c \in L(C) \cap L(B)$. Then, we test if $c \in \DEC(\Pi)$. There are three possible outcomes:
\begin{enumerate}
	\item There is a decision vector $c$ in $L(C) \cap L(B)$, and $c \in \DEC(\Pi)$: $c$ is a feasible error decision vector. The modified learning algorithm reports this feasible error decision vector $c$ to user. 
	\item There is a decision vector $c$ in $L(C) \cap L(B)$, but $c \notin \DEC(\Pi)$: $c$ is in the symmetric difference of $L(C)$ and $\DEC(\Pi)$, hence it is a valid counterexample for the classical automata learning algorithm to refine the candidate automaton $C$. 
	\item There are no decision vector in $L(C) \cap L(B)$, i.e., $L(C) \cap L(B) = \emptyset$: the modified learning algorithm poses an equivalence query $Equ(C)$ to the teacher. 
\end{enumerate}

Given a teacher that correctly answers membership queries and equivalence queries about $\DEC(\Pi)$, the modified automata learning algorithm has the following properties.
\begin{lemma}
	Assume $\DEC(\Pi)$ is a regular set. The modified automata learning algorithm eventually finds a counterexample $c \in L(B) \cap \DEC(\Pi)$ when $L(B) \cap \DEC(\Pi) \neq \emptyset$. It eventually finds a finite automaton recognizing $\DEC(\Pi)$ when $L(B) \cap \DEC(\Pi) = \emptyset$.
\end{lemma}

Note that when the program is bug-free, the behavior of the modified automata learning algorithm is identical to the classical automata learning algorithm, and therefore is still an exact automata learning algorithm. In the next section, we will go on to explain how to relax the requirements of the exact automata learning algorithm in order to retrieve a PAC automata learning algorithm that is suitable for program verification in reality. 

\section{Probably Approximately Correct Learning}\label{sec:pac_learning}

The techniques for learning automata in Section \ref{sec:learning_program_verify} assume a teacher that can answer equivalence queries. This assumption is, however, invalid in our procedure, since checking $\DEC(\Pi) = L(C)$ can be undecidable. Angluin showed in \cite{Angluin88} that if we substitute equivalence queries with sampling, we can still make statistical claims about the difference of the inferred set and the target set. 

Assume we are given the following: a probability distribution $D$ over the elements in a universe $\mathcal{U}$, and a hypothesis in the form
\begin{equation}\label{eq:pac_hypothesis}
	Prob_{w \in \mathcal{U}|D} [\neg \varphi (w)] \leq \epsilon
\end{equation}
In the hypothesis, the term $Prob_{w \in \mathcal{U}|D} [\neg \varphi (w)]$ denotes the probability that the formula $\varphi(w)$ is invalid for word $w$, which is chosen from $\mathcal{U}$ according to distribution $D$. We call $\epsilon$ the \emph{error} parameter and use the term \emph{confidence} to denote the least probability that the hypothesis \ref{eq:pac_hypothesis} is correct. We say that $\varphi(w)$ is \emph{$PAC(\epsilon, \delta)-valid$} if with confidence $\delta$, $Prob_{w \in \mathcal{U}|D} [\neg \varphi (w)] \leq \epsilon$. 

In the scenario of automata learning, the considered universe is $\Sigma^\ast$, and the target regular language is $R \subseteq \Sigma^\ast$. The task of an equivalence query $Equ(w)$ is changed from checking \emph{exact} equivalence (which we can express as checking that $\forall w \in \Sigma^\ast : w \notin R \oplus L(C)$) to checking \emph{approximate} equivalence, i.e., checking whether the formula $\varphi(w) = w \notin R \oplus L(C)$ is $PAC(\epsilon, \delta)$-valid. In other words, we check whether $Prob_{w \in \Sigma^\ast|D} [w \in R \oplus] \leq \epsilon$ with confidence $\delta$. For a fixed $R$ and a candidate automaton $C$, we call $C$ \emph{$\PAC$} if $w \notin R \oplus L(C)$ is $PAC(\epsilon, \delta)$-valid.

The teacher checks the $\PAC$ness of $C$ by picking $r$ samples according to $D$, then tests the samples to see if none of them is in $R \oplus L(C)$. A formula to determine $r$ in the equivalence query for each iteration of the learning algorithm is given by Dana Angluin in \cite{Angluin87}: for the $i$-th equivalence query of the learning algorithm, the number of samples $q_i$ needed in order to conclude that $C$ is $\PAC$ is
\begin{equation}
	q_i = \lceil \frac{1}{\epsilon} (\text{ln} \frac{1}{1-\delta} + i \text{ln}2) \rceil
\end{equation}
Since the infered set $C$ is guaranteed to be $\PAC$, this approach is termed \emph{probably approximately correct} (PAC) learning \cite{Valiant84}.