
\chapter{Introduction}\label{ch:introduction}

Formal verification on software aims to prove program properties through rigorous mathematical reasoning. Consider, for example, the statement \texttt{assert(x > 0)} in C language. The statement explicitly asserts that the value of the variable \texttt{x} is positive. If the assertion is formally verified, it \emph{cannot} be violated in any possible execution during runtime. Formal verification techniques are, however, often computationally expensive. Although sophisticated heuristics have been developed in hopes of improving scalability of the techniques, formally verifying real-world software is still considered to be impractical. 

A common practice to ensure program quality in industry is software testing. Errors in software can be detected by exploring different software behaviors via injecting various testing vectors. On the other hand, software testing cannot guarantee a program to be free from errors. Consider again the assertion \texttt{assert(x > 0)}. Unless all system behaviors are explored by the testing vectors, it is unsound to conclude that the value of \texttt{x} is always positive. Various techniques have been proposed to improve its coverage, but it is an inherent feature of software testing that it cannot establish program properties conclusively. 

In this paper, we propose two approaches to repel the issues in the practices mentioned in the above paragraphs, namely the \emph{learning-based} procedure and the \emph{sampling-based} procedure. To be more specific, the approches we propose aim to balance the scalability and coverage of existing software engineering techniques. In order to be scalable, as for software testing, our technique only expores only a subset of all program behaviors. Moreover, in the learning-based procedure we apply machine learning to generalize observed program behavior for better sementic coverage. Our techniques allow software engineers to combine scalable testing with high-coverage formal analysis, while also improving the quality assuring process. In a nutshell, we hope our work can reduce the dichotomy between formal and practical software engineering techniques. 

In our techincal setting, we assume programs are annotated with program assertions. A program assertion is a Boolean expression intended to be true every time it is encountered during program execution. Given a program with assertions, our task is to check whether 
\emph{all} assertions evaluate to true on all a=possible executions. In principle, this problem can be solved by examining all program executions. However, it is clearly not possible to inspect every execution exhaustively, since there may be infinitely many of them. A method to simplify the analysis is to group the set of program executions to paths of a control flow graph. 

A \emph{control flow graph} (CFG) is derived from the syntactic structure of a program source code. Each execution of a program corresponds to a path in its control flow graph. Consequently, one can measure the completenes of software testing on CFGs. Line coverages, for instance, gives the ratio of explored edges in the CFG of the program-under-test, while branch coverage is the ratio of explored branches in the CFG. Note that such syntactic measures of code coverage approximate program executions only very roughly. Executions that differ in the number of iterations in a simple loop have the same line and branch coverages, although their computation may be drastically different. A full syntactic code coverage does not necessarily mean all execution paths have been explored by software testing.

Observe that program executions traversing the same path in a CFG performs the same sequence of operations (possibly with different values). Consider a path corresponding to a program execution in a CFG. Such a path can be characterized by the sequence of decisions taken by the execution while traversing the condition statements in the CFG. We call a sequence of such decisions a \emph{decision vector}. A decision vector is deemed feasible if it represents one or more, possibly infinitely many, program executions, and is infeasible if it represents a sequence of branching decisions that can never occur in the program executions. To check whether all assertions evaluate to true on all executions, if suffices to check only the set of all feasible decision vectors, and examine if any of them represents an assertion-violating program execution. Although feasibility of a decision vector can be determined by using a readily available Satisfiability Modulo Theories (SMT) solver, computing and generating the exact set of feasible decision vectors is difficult in general. In the learning-based procedure, we apply algorithmic learning, in particular the framework of probably approximately correct learning, to construct a regular approximation of this set.

\begin{comment}
Program verification is a grand challenge with significant impact in computer science.
Its main difficulty is in great part due to complicated program features such as concurrent execution, \hide{of threads,} pointers, \hide{with unbounded heap size,} recursive function calls, \hide{with recursions,} and unbounded basic data types~\cite{ClarkeJS05}. Subsequently,  is extremely tedious to develop a verification algorithm thitat handles all features. Researches on program verification typically address some of these features and simplify others. Verification tools however are required to support as many features as possible. Since implementation becomes increasingly unmanageable with additional features, incorporating algorithms for all features in verification tools can be a nightmare for developers.

One way to address the implementation problem is by reduction. If verifying a new feature can be transformed to existing features, development efforts can be significantly reduced.
In this paper, we propose an algorithm to extend intraprocedural (recursion-free) program analyzers to verify recursive programs. Such analyzers supply an \emph{inductive invariant} when a program is verified to be correct an/d support program constructs such as assumptions, assertions, and nondeterministic values. Our algorithm transforms any recursive program into recursion-free ones and invokes an intraprocedural program analyzer to verify properties about the generated recursion-free programs. The verification results allow us to infer properties on the given recursive program.

Our algorithm proceeds by iterations. In each iteration, it transforms the recursive program into a recursion-free program that \emph{under-approximates} the behaviors of the original and sends the under-approximation to an intraprocedural program analyzer. If the analyzer verifies the under-approximation, purported \emph{function summaries} for recursive functions are computed. Our algorithm then transforms the original recursive program into more recursion-free programs with purported function summaries. It finally checks if purported function summaries are correct by sending these recursion-free programs to the analyzer.

Compared with other analysis algorithms for recursive programs, our approach is very lightweight. It only performs syntactic transformation and requires standard functionalities from underlying intraprocedural program analyzers. Moreover, our technique is very modular. Any intraprocedural analyzer providing proofs of inductive invariants can be employed in our algorithm. With the interface between our algorithm and program analyzers described here, incorporating recursive analysis with existing program analyzers thus only requires minimal implementation efforts. Recursive analysis hence benefits from future advanced intraprocedural analysis with little cost through our~lightweight and~modular~technique.

We implement a prototype using \textsc{CPAchecker} (over 140 thousand lines of \textsc{Java} code) as the underlying program analyzer~\cite{BeyerK11}. In our prototype, 1256 lines of \textsc{OCaml} code are for syntactic transformation and 705 lines of \textsc{Python} code for the rest of the algorithm. 270 lines among them are for extracting function summaries. Since syntactic transformation is independent of underlying program analyzers, only about 14\% of code need to be rewritten should another analyzer be employed. We compare it with program analyzers specialized for recursion in experiments. Although \textsc{CPAchecker} does not support recursion, our prototype scores slightly better than the second-place tool \textsc{Ultimate Automizer} on the benchmarks in the 2014 Competition on Software Verification~\cite{svcomp14}.
Encouraged by the result, we submitted and successfully published our approach
on Static Analysis Symposium~\cite{ChenHTWW14}.
We further participated the 2015 Competition on Software
Verification~\cite{svcomp15} under the name \textsc{CPArec}~\cite{ChenHTWW15}.

\hide{
Notice that in order to simplify the implementation effort, we turned off important optimizations such as adjust block encoding provided in \textsc{CPAchecker}, the performance of the prototype can be even better with those optimizations turned on.
}

\noindent
\textbf{Organization:}
Chapter~\ref{ch:related} describes related works.
Preliminaries are given in Chapter~\ref{ch:preliminaries}.
We give an overview of our technique in Chapter~\ref{ch:overview}.
Technical contributions are presented in Chapter~\ref{ch:proving-via-transformation}.
Chapter~\ref{ch:experiments} reports experimental results.
Finally, some insights and improvements are discussed in Chapter~\ref{ch:conclusion}.

\hide{
Difference compared with Whale
In general, Whale tries to extend Lazy Abstraction to verify interprocedural program.
It mainly constructs an iARG with path conditions that can encode function calls and check for reachability of bug.
Also it introduces an extended covering relation over summaries of function calls to deal with recursion.
Our work, however, uses bounded times of unwinding to construct paths across functions,
and we find reasonable summaries for recursive functions when proving safety of program.

For guessing summaries, both Whale and our work use under-approximation of function calls.
In Whale, the under-approximation of a function call is constructed through exploring paths without function call in the called function.
In our work, the function call is unwound and transformed, and the exploration is achieved by the program analyzer we used.
With our method, we can create more precise under-approximation by unwinding more times before transforming.

For proving summaries, Whale and our work apply the Hoare rule of recursion. Whale defines the covering relation between summaries upon Hoare rule of consequence for proving. Our work, in other way, directly proves by the used program analyzer.
}

\end{comment}
